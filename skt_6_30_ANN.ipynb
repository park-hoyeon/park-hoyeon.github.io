{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNsyHr9bjPmZPkZAFDMZwwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park-hoyeon/park-hoyeon.github.io/blob/master/skt_6_30_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  심층 신경망(Deep Neural Network)\n",
        "= 딥러닝\n",
        "- \t입력층에서\t출력층\t방향으로\t이동하면서\t각\t입력에\t해당하는\t가중치를\t곱하고,\t활성화\t함수를\t거쳐서\t최총\t출력까지의\t과정을\t순전파(Forward\tPropagation,\tFeed\tForward)\n",
        "라고\t한다.\n",
        "- \t\t순전파에\t의한\t예측과\t실제값(정답)의\t오차를\t출력층에서\t입력층\t방향으로\t전파시키며\t각\t층의\t가중치를\t업데이트하는\t과정을\t역전파(Back-propagation)라\t한다."
      ],
      "metadata": {
        "id": "rDGllyIQPbRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzVLXrphG2jH"
      },
      "outputs": [],
      "source": [
        " # 단순 선형회귀를 완전연결층을 사용해 학습한다.\n",
        " import numpy as np\n",
        " X = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0])\n",
        " y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
        " print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#산점도를\t그려서\t데이터의\t분포를\t확인\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X, y, 'ro')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_YsdGF2CL4l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#케라스를\t사용해서\t유닛(Unit)이\t한\t개인\t모델을\t만든다.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "layers.Dense(1, input_shape=(1,)),])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "G1rIukzSMR26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델의 컴파일\n",
        "#Keras는\t모델의\t학습전에\t컴파일\t과정을\t거친다.\t손실함수와\t옵티마이저\t그리고\t메트릭스를\t설정한다.\n",
        "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "4_bXOjn8Ow7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습\n",
        "history = model.fit(X, y, epochs=30, verbose=1)"
      ],
      "metadata": {
        "id": "0vMzeof6RzAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 결과의 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "krgL-7RNR5bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#예측하기\n",
        "import numpy as np\n",
        "X_test = np.array([[10.0]])\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "pYSQqdALSI-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치의 출력\n",
        "w, b = model.get_weights()\n",
        "print('w :', w)\n",
        "print('b :', b)"
      ],
      "metadata": {
        "id": "k-92TKm6SdYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측값의 시각화\n",
        "y_pred = model.predict(X.reshape(-1, 1))\n",
        "y_pred"
      ],
      "metadata": {
        "id": "FOUmEKcsSjww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화 - 원본 데이터와 예측된 직선을 그래프로 표현\n",
        "plt.plot(X, y, 'ro')\n",
        "plt.plot(X, y_pred, 'g--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRvx08CgS1JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다중 선형회귀\n"
      ],
      "metadata": {
        "id": "nshms9cQYlrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import pandas as pd\n",
        " mpg = pd.read_csv('./auto-mpg.csv', na_values = \"?\")\n",
        " mpg.head()"
      ],
      "metadata": {
        "id": "KbpaIQXDYj2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 정보 확인\n",
        "mpg.info()"
      ],
      "metadata": {
        "id": "pepNqwoCZaOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결측치 개수 확인하기\n",
        "mpg.isna().sum(axis=0)"
      ],
      "metadata": {
        "id": "cnIR2xcuZktB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# horsepower의 6개의 결측치 확인하기\n",
        "df = mpg[mpg['horsepower'].isna()]\n",
        "df"
      ],
      "metadata": {
        "id": "iQ3lzWJBZ4yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리하기 - 결측치를 호함한 행을 삭제한 후 확인하기\n",
        "\n",
        "mpg = mpg.dropna()\n",
        "mpg.isna().sum(axis=0)"
      ],
      "metadata": {
        "id": "kInmFd8xaU7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성과 레이블 분리 - 특성 데이터로 사용할 컬럼 데이터만 추출 (mpg와 car name만 날리고 나머지 사용함)\n",
        "X_data = mpg.drop(['mpg', 'car name'], axis=1)\n",
        "X_data.head()"
      ],
      "metadata": {
        "id": "CF3jYPEZajuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#레이블 데이터로 사용할 연비를 저장 (y가 연비 = 연비를 예측하고 싶음)\n",
        "y_data = mpg['mpg']\n",
        "y_data.head()"
      ],
      "metadata": {
        "id": "gu2NKAYua36I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 분리 (train test 스플릿 하는 이유 = overfiting 피하기 위함, 학습되지 않은 데이터로 모델을 확인하기 위함)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42) #20% 데이터를 테스트데이터로 분리함\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)\n",
        "# 입력하는 것은 7개의 값들.(shape보고 알 수 있어야 함)"
      ],
      "metadata": {
        "id": "-qKb-mejbFAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정규화 - (0~1값으로 정규화 하는 이유: )\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "print(X_train_s[:5])"
      ],
      "metadata": {
        "id": "TEanoZFtbQ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 넘파이 배열로 변환 - 모델의 입력 데이터로 사용하기 위해 넘파이의 ndarray 타입으로 변경\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "print(type(X_train_s), type(y_train))"
      ],
      "metadata": {
        "id": "A4ZZAYYmb2GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기 - 2개의 은닉층과 출력층을 가진 모델을 구성\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "layers.Dense(64, activation='relu', input_shape=(7,)),#64개의 퍼셉트론이 생긴 것/ input_shape 7 이게 일치하지 않으면 에러남. (7개의 피처)\n",
        "layers.Dense(64, activation='relu'),#default-앞에 만들어진것과 같음. = 연결이 된 것\n",
        "layers.Dense(1) #한개짜리 뉴런을 만듦. (나온 64개와 1개짜리와 다 연결된 형태임)\n",
        " ])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CglSN23kcJFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 컴파일 - 손실함수와\t최적화\t방법\t그리고\t매트릭스를\t설정\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam',\n",
        "      metrics=['mae', 'mse'])"
      ],
      "metadata": {
        "id": "h8er9xcIc1H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "history = model.fit(X_train_s, y_train,\n",
        "epochs=EPOCHS,\n",
        "batch_size=BATCH_SIZE,\n",
        "validation_split = 0.2, verbose=1)"
      ],
      "metadata": {
        "id": "5uJm3m1mdjZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history\t객체에\t저장된\t데이터의\t키를\t확인\n",
        "history.history.keys()"
      ],
      "metadata": {
        "id": "kc3RN5nmdseB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 곡선 - 손실값의 변화와 mae 값의 변화를 시각화한다.\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        " hist = pd.DataFrame(history.history)\n",
        " hist['epoch'] = history.epoch\n",
        "\n",
        " plt.figure(figsize=(16,8))\n",
        " plt.subplot(1,2,1)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Mean Abs Error [MPG]')\n",
        " plt.plot(hist['epoch'], hist['mae'], label='Train Error')\n",
        " plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error')\n",
        " plt.ylim([0,5])\n",
        " plt.legend()\n",
        "\n",
        " plt.subplot(1,2,2)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Mean Square Error [$MPG^2$]')\n",
        " plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        " plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n",
        " plt.ylim([0,20])\n",
        " plt.legend()\n",
        " plt.show()\n",
        "plot_history(history)\n",
        "\n",
        "# valudation data? = 평가용으로만 진행."
      ],
      "metadata": {
        "id": "bt6h-URIdyCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터로 평가\n",
        "loss, mae, mse = model.evaluate(X_test_s, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "cSTSCr_Gd5S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트\t세트를\t사용해서\t연비(mpg)를\t예측\n",
        "y_pred = model.predict(X_test_s)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "I1EZroKzeJZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과 시각화\n",
        "y_pred = y_pred.flatten()\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "plt.plot([-100, 100], [-100, 100])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tBKwM1lFeLjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차의 분포 시각화\n",
        "error = y_test - y_pred\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [MPG]\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OL4L4pKGel6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN(Neural Network) 짚어가기\n",
        "https://working-penguin-c63.notion.site/NN-Neural-Network-221ec508cf3c8078b17ad2e60fc15634\n"
      ],
      "metadata": {
        "id": "tijUSmSDlBOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이진 분류(Binary Classification)\n",
        "### 피마 인디언 데이터를 사용해 당뇨병 이진분류를 실행해보자."
      ],
      "metadata": {
        "id": "6Ff0UTXwerpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas  as pd\n",
        "diabetes = pd.read_csv('./diabetes.csv')\n",
        "diabetes.head()"
      ],
      "metadata": {
        "id": "tMQimiRmez4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정보 확인\n",
        "diabetes.info()"
      ],
      "metadata": {
        "id": "rO-i2wuSrQu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 유무 확인\n",
        "diabetes.isna().sum(axis=0)"
      ],
      "metadata": {
        "id": "66R1NsYjrthN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이상치 확인\n",
        "# 당뇨병 데이터에는 0이 될 수 없는 값이 0인 이상치가 있다.\n",
        "cols = ['Glucose', 'BloodPressure', 'SkinThickness',\n",
        "'Insulin', 'BMI']\n",
        "(diabetes[cols] == 0).sum(axis=0)"
      ],
      "metadata": {
        "id": "tlwVzmLqyTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(x='Outcome', data = diabetes)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wvJ7e1zZz3A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 당뇨병 범주별 개수를 확인한다.\n",
        "diabetes['Outcome'].value_counts()"
      ],
      "metadata": {
        "id": "QfkDQ8qTz3NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성과 레이블 분리\n",
        "X_data = diabetes.drop(['Outcome'], axis=1)\n",
        "X_data.head()"
      ],
      "metadata": {
        "id": "Qdv074d40JAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블로 사용할 'Outcome' 컬럼 데이터를 추출한다.\n",
        "y_data = diabetes['Outcome']\n",
        "y_data.head()"
      ],
      "metadata": {
        "id": "eRV-4GLx0fOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,\n",
        "                                  test_size=0.2, random_state=42)\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "mUsExbP30mnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 처리 - 이상치에 평균 적용\n",
        "import numpy as np\n",
        "def impute_zero(data, col):\n",
        "  df = data.loc[data[col] != 0, col]\n",
        "  avg = np.sum(df) / len(df)\n",
        "  k = len(data.loc[ data[col] == 0, col])\n",
        "  data.loc[ data[col] == 0, col ] = avg\n",
        "  print('%s : fixed %d, mean: %.3f' % (col, k, avg))\n",
        "\n",
        "for col in cols:\n",
        "  impute_zero(X_train, col)"
      ],
      "metadata": {
        "id": "-dXiZqHs1Hdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 확인 - 이상치가 존재하는지 다시 확인\n",
        "(X_train[cols] == 0).sum(axis=0)"
      ],
      "metadata": {
        "id": "cRQpY4Js1f1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터의 이상치 처리\n",
        "for col in cols:\n",
        "  impute_zero(X_test, col)"
      ],
      "metadata": {
        "id": "Fs4BWaGu1lp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정규화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "print(X_train_s[:5])"
      ],
      "metadata": {
        "id": "KHmYSk9o19Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 넘파이 배열로 타입 변경\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "print(type(y_train), type(y_test))"
      ],
      "metadata": {
        "id": "PFe8FDr7JJED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기 - keras를 사용해 모델 구성\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Dense(12, input_dim=8, activation='relu'))\n",
        "  model.add(layers.Dense(8, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GbSDIkT7JPKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 컴파일 - binary_crossentropy\n",
        "model.compile(loss='binary_crossentropy',\n",
        "       optimizer='adam',\n",
        "        metrics=['acc'])"
      ],
      "metadata": {
        "id": "jVyf5Es0JoD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 16 #한번에 처리할 수 있는 데이터의 묶음 (전체 데이터 중에~~)\n",
        "history = model.fit(X_train_s, y_train,\n",
        "           epochs=EPOCHS,\n",
        "           batch_size=BATCH_SIZE,\n",
        "           validation_split = 0.2,\n",
        "           verbose=1)\n"
      ],
      "metadata": {
        "id": "3LmisnbuJx2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 곡선 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        " hist = pd.DataFrame(history.history)\n",
        " hist['epoch'] = history.epoch\n",
        " plt.figure(figsize=(16,8))\n",
        " plt.subplot(1,2,1)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Loss')\n",
        " plt.plot(hist['epoch'], hist['loss'], label='Train Loss')\n",
        " plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Loss')\n",
        " plt.legend()\n",
        " plt.subplot(1,2,2)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Accuracy')\n",
        " plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')\n",
        " plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')\n",
        " plt.legend()\n",
        "\n",
        " plt.show()\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "IycNEf3BJ7XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 위 결과에서는 오버피팅(과적합)이 일어났음.\n",
        "### 오른쪽 그래프는 Val Accuracy가 상승하다가 비교적 일정함\n"
      ],
      "metadata": {
        "id": "9FNtYYCyNhS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이전 실습에서 사용한 데이터를 사용해, 콜백 함수를 적용해보자."
      ],
      "metadata": {
        "id": "LiVIj5F2Khqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성과 컴파일\n",
        "from tensorflow import keras\n",
        "\n",
        "model = build_model()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "       optimizer='adam',\n",
        "        metrics=['acc'])"
      ],
      "metadata": {
        "id": "V-WZilPNKfyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콜백 함수 - 이번 예제에서 사용하는 콜백은 학습이 진척이 없을 때 조기 종료하는 기능\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "nh_dn_VCKwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 16\n",
        "history = model.fit(X_train_s, y_train,\n",
        "epochs=EPOCHS,\n",
        "batch_size=BATCH_SIZE,\n",
        "validation_split = 0.2,\n",
        "callbacks=[early_stop],\n",
        "verbose=1)"
      ],
      "metadata": {
        "id": "lrkrM6VjK9Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Z2L9lKuSEwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "프로젝트\n"
      ],
      "metadata": {
        "id": "Gu_jk7kuR5ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "value = pd.read_csv('./california_housing_train.csv', na_values = \"?\")\n",
        "display(value.head())"
      ],
      "metadata": {
        "id": "XOglCuOjR7Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value.info()"
      ],
      "metadata": {
        "id": "ebsuqA7VSCiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate features and target\n",
        "X = value.drop('median_house_value', axis=1)\n",
        "y = value['median_house_value']\n",
        "\n",
        "# Apply Standard Scaling to features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "nuVIHEKZSGId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.preprocessing import StandardScaler\n",
        " scaler = StandardScaler()\n",
        " X_train_s = scaler.fit_transform(X_train)\n",
        " X_test_s = scaler.transform(X_test)\n",
        " print(X_train_s[:5])"
      ],
      "metadata": {
        "id": "i7qMUnSlTrgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " y_train = y_train.values\n",
        " y_test = y_test.values\n",
        " print(type(y_train), type(y_test))"
      ],
      "metadata": {
        "id": "IPACBbocTtyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "layers.Dense(64, activation='relu', input_shape=(8,)), # Changed input_shape from 7 to 8\n",
        " layers.Dense(64, activation='relu'),\n",
        " layers.Dense(1)\n",
        " ])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UdYqmL5LSI2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 컴파일\n",
        "model.compile(loss='mse', optimizer='adam',\n",
        "      metrics=['mae', 'mse'])"
      ],
      "metadata": {
        "id": "G_vjm98ISKWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "history = model.fit(X_train_s, y_train,\n",
        "epochs=EPOCHS,\n",
        "batch_size=BATCH_SIZE,\n",
        " validation_split = 0.2,\n",
        "verbose=1)"
      ],
      "metadata": {
        "id": "LuEErCRJSnuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss') # Changed label\n",
        "    plt.plot(hist['epoch'], hist['loss'], label='Train Loss') # Changed metric to loss\n",
        "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss') # Changed metric to val_loss\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy') # Changed label\n",
        "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy') # Changed metric to accuracy\n",
        "    plt.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy') # Changed metric to val_accuracy\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aa_Ev4quT-mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8caf818"
      },
      "source": [
        "# Task\n",
        "Analyze the provided Python code for a multi-layer perceptron model using the \"california_housing_train.csv\" dataset, which aims to predict `median_house_value`. The current model exhibits high loss values. Based on the user's request and suggested approaches (changing multi-layer perceptron model - optimizer, etc., testing on test values, using MAE as the evaluation metric), propose and implement modifications to the model and training process to reduce the loss and improve performance. Evaluate the performance of the modified model(s) using MAE on the test set and report the results. The analysis and modifications should focus on the provided code structure and the suggested techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00dafb9e"
      },
      "source": [
        "# 모델 구조 변경 (실험 1) - 은닉층 뉴런 수 증가\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model_experiment1():\n",
        "  model = keras.Sequential([\n",
        "      layers.Dense(128, activation='relu', input_shape=(X_train_s.shape[1],)), # 뉴런 수 128개로 변경\n",
        "      layers.Dense(128, activation='relu'), # 뉴런 수 128개로 변경\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model_exp1 = build_model_experiment1()\n",
        "model_exp1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5402bfd"
      },
      "source": [
        "# 모델 컴파일 (실험 1)\n",
        "model_exp1.compile(loss='mae', optimizer='adam',\n",
        "              metrics=['mae', 'mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShdcKq5hW9Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7812bb8"
      },
      "source": [
        "# 모델 학습 (실험 1)\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "history_exp1 = model_exp1.fit(X_train_s, y_train,\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          validation_split=0.2,\n",
        "                          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecdd9ff1"
      },
      "source": [
        "# 학습 결과 시각화 (실험 1)\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Abs Error') # Changed label to be more general\n",
        "    plt.plot(hist['epoch'], hist['mae'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mae'], label='Val Error')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error') # Changed label to be more general\n",
        "    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mse'], label='Val Error')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_exp1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "def build_model():\n",
        "  model=keras.Sequential()\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(32, activation='relu'))\n",
        "\n",
        "  model.add(layers.Dense(1))\n",
        "  return model"
      ],
      "metadata": {
        "id": "SKzPSmbAYu5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 (실험 1)\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "history_exp1 = model_exp1.fit(X_train_s, y_train,\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          validation_split=0.2,\n",
        "                          verbose=1)"
      ],
      "metadata": {
        "id": "e2SUTvz-ZH0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8URSq3UYbY9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04acfa6e"
      },
      "source": [
        "# 테스트 데이터 로드\n",
        "test_value = pd.read_csv('./california_housing_test.csv', na_values=\"?\")\n",
        "display(test_value.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b835005"
      },
      "source": [
        "# 테스트 데이터 전처리 (훈련 데이터와 동일한 스케일러 사용)\n",
        "X_test_eval = test_value.drop('median_house_value', axis=1)\n",
        "y_test_eval = test_value['median_house_value']\n",
        "\n",
        "# 훈련 데이터 스케일링에 사용된 scaler 객체를 사용하여 테스트 데이터를 스케일링합니다.\n",
        "X_test_eval_s = scaler.transform(X_test_eval)\n",
        "y_test_eval = y_test_eval.values # Convert to numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4ad8ed"
      },
      "source": [
        "# 모델 평가 (실험 1 모델 사용)\n",
        "loss_eval, mae_eval, mse_eval = model_exp1.evaluate(X_test_eval_s, y_test_eval, verbose=2)\n",
        "\n",
        "print(f'Evaluation MAE on Test Data: {mae_eval}')\n",
        "print(f'Evaluation MSE on Test Data: {mse_eval}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "181f7c54"
      },
      "source": [
        "# 테스트 데이터로 예측 및 시각화 (실험 1 모델 사용)\n",
        "y_pred_eval = model_exp1.predict(X_test_eval_s)\n",
        "y_pred_eval = y_pred_eval.flatten()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test_eval, y_pred_eval)\n",
        "plt.xlabel('True Values (Test Data)')\n",
        "plt.ylabel('Predictions (Test Data)')\n",
        "plt.title('True vs Predicted Values on Test Data (Experiment 1)')\n",
        "plt.xlim([0, plt.xlim()[1]])\n",
        "plt.ylim([0, plt.ylim()[1]])\n",
        "plt.plot([-100000, 600000], [-100000, 600000], color='red', linestyle='--')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "# 1) 데이터 불러오기\n",
        "df = pd.read_csv('california_housing_train.csv')\n",
        "test_df = pd.read_csv('california_housing_test.csv')\n",
        "\n",
        "# 2) 입력(X), 출력(y) 나누기\n",
        "train_x = df.drop('median_house_value', axis=1)\n",
        "train_y = df['median_house_value']\n",
        "test_x = test_df.drop('median_house_value', axis=1)\n",
        "test_y = test_df['median_house_value']  # 성능 측정용 정답값\n"
      ],
      "metadata": {
        "id": "DVvmHG4PbbXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) 정규화\n",
        "scaler = StandardScaler()\n",
        "train_x_scaled = scaler.fit_transform(train_x)\n",
        "test_x_scaled = scaler.transform(test_x)\n"
      ],
      "metadata": {
        "id": "jy9LfHu8bhwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=[train_x.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n"
      ],
      "metadata": {
        "id": "PS74gbv-blvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_x_scaled, train_y,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=256\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "xaUxFv9lbqwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이진 분류 - 콜백\t함수를\t적용"
      ],
      "metadata": {
        "id": "h-a4yjrOlUaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성과 컴파일\n",
        "from tensorflow import keras\n",
        "model = build_model()\n",
        "model.compile(loss='binary_crossentropy',\n",
        " optimizer='adam',\n",
        " metrics=['acc'])"
      ],
      "metadata": {
        "id": "c7LM5SoilmMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 콜백 함수\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "                                 monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "eNwgXtbGlsx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 16\n",
        "history = model.fit(X_train_s, y_train,\n",
        "epochs=EPOCHS,\n",
        "batch_size=BATCH_SIZE,\n",
        "validation_split = 0.2,\n",
        "callbacks=[early_stop],\n",
        "verbose=1)"
      ],
      "metadata": {
        "id": "GjtFC8gZmJm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측\n",
        "y_pred = model.predict(X_test_s)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "46P_JW2bmQe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과 확인\n",
        "y_pred= (y_pred >0.5).astype(np.int32).flatten()\n",
        "y_pred"
      ],
      "metadata": {
        "id": "vsRT3zyQmchy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 혼동행렬 시각화\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cf_matrix, annot=True, cbar=False)\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soN72LITmixo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다중 분류"
      ],
      "metadata": {
        "id": "xwI1Hwofmpqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas  as pd\n",
        "penguins = pd.read_csv('./penguins.csv')\n",
        "penguins.head()"
      ],
      "metadata": {
        "id": "45Iezqq7moCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 펭귄의 종류 확인\n",
        "penguins['Species'].value_counts()"
      ],
      "metadata": {
        "id": "NRKNF_jjmzjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정보 확인\n",
        "penguins.info()"
      ],
      "metadata": {
        "id": "M4JZcZtWnEIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "penguins.isna().sum(axis=0)"
      ],
      "metadata": {
        "id": "jtAoELg7nX5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리\n",
        "penguins = penguins.dropna()\n",
        "penguins.isna().sum(axis=0)"
      ],
      "metadata": {
        "id": "K2yhu1KwnfsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 속성과 레이블 분리\n",
        "X = penguins.drop(['Species'], axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "ccsWcqkjnjhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 데이터 분리\n",
        "y = penguins[\"Species\"]\n",
        "y.head()"
      ],
      "metadata": {
        "id": "rnBIta3entT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "tnUlLdeCn01F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 사용할 칼럼 추출\n",
        "df = X_train[['Culmen Length(mm)','Culmen Depth(mm)',\n",
        " 'Flipper Length(mm)', 'Body Mass(g)']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3YAt7pNNn5_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_s = scaler.fit_transform(df)\n",
        "df_s"
      ],
      "metadata": {
        "id": "jiOS_WR8oB_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임 생성\n",
        "dfX = pd.DataFrame(df_s,\n",
        "                   columns=['bill_length_mm','bill_depth_mm',\n",
        " 'flipper_length_mm', 'body_mass_g'])\n",
        "dfX.head()"
      ],
      "metadata": {
        "id": "2uZVSNrqoFZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 인코딩 - map()함수를    사용해서    섬이름     문자열을    숫자      데이터로    변경\n",
        "dfX['island']= X_train['Island'].map(\n",
        " {'Biscoe': 0, 'Dream': 1, 'Torgersen': 2}).reset_index(drop = True)\n",
        "dfX.head(5)"
      ],
      "metadata": {
        "id": "Bh2LqOwtoLnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성별 문자열을 인코딩\n",
        "dfX['sex'] = X_train['Sex'].map(\n",
        " {'FEMALE': 0, 'MALE': 1}).reset_index(drop = True)\n",
        "dfX.head(10)"
      ],
      "metadata": {
        "id": "SQwgfWG1oWn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 펭귄의 종류 문자열을 숫자로 인코딩\n",
        "dfy = y_train.map(\n",
        "   {'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}).reset_index(drop = True)\n",
        "dfy"
      ],
      "metadata": {
        "id": "itChyMp0ogsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원-핫 인코딩\n",
        "from tensorflow.keras import utils\n",
        "y_train = utils.to_categorical(dfy)\n",
        "y_train[:20]"
      ],
      "metadata": {
        "id": "pm-n_vW4onLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "   layers.Dense(32, activation='relu', input_shape=(6,)),\n",
        "   layers.Dense(16, activation='relu'),\n",
        "   layers.Dense(8, activation='relu'),\n",
        "   layers.Dense(3, activation='softmax')   ## 다중 분류\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oceRMAFPovTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 컴파일\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "optimizer='adam',\n",
        "metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "w8Jv-2l6o0lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 학습\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "history = model.fit(dfX, y_train,\n",
        "epochs=EPOCHS,\n",
        "batch_size=BATCH_SIZE,\n",
        "validation_split=0.2)"
      ],
      "metadata": {
        "id": "-eziK9DIo4JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 곡선\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "J-dYq6xMpDMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 펭귄\n"
      ],
      "metadata": {
        "id": "0ZZ_Z08L1fWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "# 1. 데이터 불러오기\n",
        "train_df = pd.read_csv('./penguins_train.csv')\n",
        "test_df = pd.read_csv('./penguins_test.csv')\n",
        "\n",
        "# 2. 결측치 제거\n",
        "train_df.dropna(inplace=True)\n",
        "test_df.dropna(inplace=True)\n",
        "\n",
        "# 3. 입력(X), 출력(y) 나누기\n",
        "X_train = train_df.drop('Body Mass(g)', axis=1)\n",
        "y_train = train_df['Body Mass(g)']\n",
        "X_test = test_df.drop('Body Mass(g)', axis=1)\n",
        "y_test = test_df['Body Mass(g)']\n",
        "\n",
        "# 4. 전처리: 수치형 + 범주형 처리\n",
        "numeric_features = ['Culmen Length(mm)', 'Culmen Depth(mm)', 'Flipper Length(mm)']\n",
        "categorical_features = ['Species', 'Island', 'Sex']\n",
        "\n",
        "# Fit OneHotEncoder on combined unique values from train and test sets\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "ohe.fit(pd.concat([X_train[categorical_features], X_test[categorical_features]], axis=0))\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', ohe, categorical_features) # Use the fitted OneHotEncoder\n",
        "])\n",
        "\n",
        "# 5. 전처리 후 모델 구성\n",
        "def build_model(input_dim):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=[input_dim]))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # 회귀 출력\n",
        "    return model\n",
        "\n",
        "# 6. 전처리 수행\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "# 7. 모델 학습\n",
        "model = build_model(X_train_processed.shape[1])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "history = model.fit(X_train_processed, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "# 8. 테스트 평가\n",
        "loss, mae = model.evaluate(X_test_processed, y_test)\n",
        "print(f\"\\n✅ 테스트 MAE: {mae:.2f}g, MSE: {loss:.2f}g²\")\n",
        "\n",
        "# 9. 예측 보기\n",
        "predictions = model.predict(X_test_processed[:5])\n",
        "for i in range(5):\n",
        "    print(f\"예측값: {predictions[i][0]:.1f}g / 실제값: {y_test.iloc[i]:.1f}g\")"
      ],
      "metadata": {
        "id": "B4j4_PnO1g78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=[X_train_processed.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # 회귀 출력\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n",
        "\n",
        "# 7. 학습\n",
        "history = model.fit(X_train_processed, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "LVBUiUWF2ICE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_shape=[X_train_processed.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n",
        "\n",
        "# 7. 학습\n",
        "history = model.fit(X_train_processed, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=200,\n",
        "                    batch_size=64,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "200XzGry2b1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=[X_train_processed.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # 회귀 출력\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n",
        "\n",
        "# 7. 학습\n",
        "history = model.fit(X_train_processed, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=300,\n",
        "                    batch_size=64,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "YDaMwn2e2vfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 1. 모델 정의 (기존과 동일)\n",
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=[X_train_processed.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))  # 추가 Dropout\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # 회귀 출력\n",
        "    return model\n",
        "\n",
        "# 2. 모델 컴파일\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n",
        "\n",
        "# 3. 콜백 정의 (과적합 방지용)\n",
        "early_stop = EarlyStopping(monitor='val_mae', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mae', patience=10, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "# 4. 모델 학습\n",
        "history = model.fit(X_train_processed, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=500,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[early_stop, reduce_lr],\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "VUpVIvMY5IXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 1. 모델 정의 (기존과 동일)\n",
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=[X_train_processed.shape[1]]))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))  # 추가 Dropout\n",
        "\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # 회귀 출력\n",
        "    return model\n",
        "\n",
        "# 2. 모델 컴파일\n",
        "model = build_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mae',\n",
        "              metrics=['mae', 'mse'])\n",
        "\n",
        "# 3. 콜백 정의 (과적합 방지용)\n",
        "early_stop = EarlyStopping(monitor='val_mae', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mae', patience=10, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "# 4. 모델 학습\n",
        "history = model.fit(X_train_processed, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=500,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[early_stop, reduce_lr],\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "8U_ILhLl6Qdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}