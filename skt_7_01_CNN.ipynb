{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNE0Q0cj/2I/ahWXCYtuaOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park-hoyeon/park-hoyeon.github.io/blob/master/skt_7_01_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA3mKFbhN1hF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì´ë¯¸ì§€ í™•ì¸\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover',\n",
        "               'Dress', 'Coat', 'Sandal',\n",
        "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "samples = np.random.randint(len(X_train), size=9)\n",
        "plt.figure(figsize = (8, 6))\n",
        "for i, idx in enumerate(samples):\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.imshow(X_train[idx], cmap = 'gray')\n",
        "  plt.title(class_names[y_train[idx]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8X0owUSjVFF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²€ì¦ìš© ë°ì´í„°ë¥¼ í›ˆë ¨ìš© ë°ì´í„°ì—ì„œ ë¶„ë¦¬í•œë‹¤.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size = 0.3, random_state = 42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "0-HTPy7qVTkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì •ê·œí™”\n",
        "import numpy as np\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_val = X_val.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "print(np.max(X_train), np.min(X_train))"
      ],
      "metadata": {
        "id": "1Ks3fX1fVan9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° Shape í™•ì¸\n",
        "print('X_train : ', X_train.shape)\n",
        "print('X_val : ', X_val.shape)\n",
        "print('X_test : ', X_test.shape)"
      ],
      "metadata": {
        "id": "BGRu3FWoWJGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì±„ë„ ì¶• ì¶”ê°€\n",
        "import tensorflow as tf\n",
        "X_train = X_train[..., tf.newaxis]\n",
        "X_val = X_val[..., tf.newaxis]\n",
        "X_test = X_test[..., tf.newaxis]\n",
        "print('X_train : ', X_train.shape)\n",
        "print('X_val : ', X_val.shape)\n",
        "print('X_test : ', X_test.shape)"
      ],
      "metadata": {
        "id": "cy2z_EGzWhbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Conv2D(filters=16, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu',\n",
        "        input_shape=(28, 28, 1)))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2)) #rgbì´ë¯¸ì§€ëŠ” 3ì°¨ì›ì´ë‹ˆê¹Œ 2Dë¡œ ì•ˆë¨.\n",
        "  model.add(layers.Conv2D(filters=32, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(filters=64, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(10, activation = 'softmax'))\n",
        "  return model\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "XcXH5x4gWwWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ì˜ ì»´íŒŒì¼ - ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì˜ í•™ìŠµì€ ì†ì‹¤í•¨ìˆ˜ë¡œ 'categorical_crossentropy'ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam,\n",
        "       loss = 'categorical_crossentropy',\n",
        "       metrics=['acc'])\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "# Make sure to run the cell with one-hot encoding for y_train_oh and y_val_oh before this cell.\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "           epochs = EPOCHS,\n",
        "           batch_size = BATCH_SIZE,\n",
        "           validation_data = (X_val, y_val_oh),\n",
        "           verbose = 1)"
      ],
      "metadata": {
        "id": "x3pUHYI_XmUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a513f0fd"
      },
      "source": [
        "# íƒ€ê²Ÿ ë°ì´í„°ì˜ ì›-í•« ì¸ì½”ë”©\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_oh = to_categorical(y_train)\n",
        "y_val_oh = to_categorical(y_val)\n",
        "y_test_oh = to_categorical(y_test)\n",
        "\n",
        "print('y_train_oh : ', y_train_oh.shape)\n",
        "print('y_val_oh : ', y_val_oh.shape)\n",
        "print('y_test_oh : ', y_test_oh.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "def plot_history(history):\n",
        " hist = pd.DataFrame(history.history)\n",
        " hist['epoch'] = history.epoch\n",
        " plt.figure(figsize=(16,8))\n",
        " plt.subplot(1,2,1)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Loss')\n",
        " plt.plot(hist['epoch'], hist['loss'], label='Train Loss')\n",
        " plt.plot(hist['epoch'], hist['val_loss'],label = 'Val Loss')\n",
        " plt.legend()\n",
        " plt.subplot(1,2,2)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Accuracy')\n",
        " plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')\n",
        " plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')\n",
        " plt.legend()\n",
        " plt.show() # Add plt.show() here\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "q3ob1a_nbJxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì™¼ìª½ ê·¸ë˜í”„ëŠ” ì¢‹ì§€ ì•Šì€ ê²°ê³¼."
      ],
      "metadata": {
        "id": "QoqxjMWFn0dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë“œë¡­ì•„ì›ƒ(Dropout)"
      ],
      "metadata": {
        "id": "SgYHHDC-iJgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë“œë¡­ì•„ì›ƒ ë ˆë¦¬ì–´ë¥¼ ì¶”ê°€í•œ ëª¨ë¸ êµ¬ì„±\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_dropout_model():\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv2D(filters=16, kernel_size= 3,\n",
        "         strides=(1, 1), padding='same', activation='relu',\n",
        "         input_shape=(28, 28, 1)))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(filters=32, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(filters=64, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "  return model\n",
        "model = build_dropout_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KQ61MEfEkxPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "viSlhMozn42k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
        "y_pred_argmax[:10]\n"
      ],
      "metadata": {
        "id": "J0m23LyyoBe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
        "from sklearn.metrics import accuracy_score, precision_score,\n",
        "recall_score, f1_score\n",
        "def print_score(y_test, y_pred):\n",
        " print('accuracy: %.3f' % (accuracy_score(y_test, y_pred)))\n",
        " print('precision: %.3f' % (precision_score(y_test, y_pred, average='macro')))\n",
        " print('recall_score: %.3f' % (recall_score(y_test, y_pred, average='macro')))\n",
        " print('f1_score: %.3f' % (f1_score(y_test, y_pred, average='macro')))"
      ],
      "metadata": {
        "id": "e7FZ7xXCoK6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1812da01"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fca850"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì´ë¯¸ì§€ í™•ì¸\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover',\n",
        "               'Dress', 'Coat', 'Sandal',\n",
        "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "samples = np.random.randint(len(X_train), size=9)\n",
        "plt.figure(figsize = (8, 6))\n",
        "for i, idx in enumerate(samples):\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.imshow(X_train[idx], cmap = 'gray')\n",
        "  plt.title(class_names[y_train[idx]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ce84123"
      },
      "source": [
        "# ê²€ì¦ìš© ë°ì´í„°ë¥¼ í›ˆë ¨ìš© ë°ì´í„°ì—ì„œ ë¶„ë¦¬í•œë‹¤.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size = 0.3, random_state = 42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9f33a45"
      },
      "source": [
        "# ì´ë¯¸ì§€ ë°ì´í„°ì˜ ì •ê·œí™”\n",
        "import numpy as np\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_val = X_val.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "print(np.max(X_train), np.min(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94262879"
      },
      "source": [
        "# ì±„ë„ ì¶• ì¶”ê°€\n",
        "import tensorflow as tf\n",
        "X_train = X_train[..., tf.newaxis]\n",
        "X_val = X_val[..., tf.newaxis]\n",
        "X_test = X_test[..., tf.newaxis]\n",
        "print('X_train : ', X_train.shape)\n",
        "print('X_val : ', X_val.shape)\n",
        "print('X_test : ', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5f98349"
      },
      "source": [
        "# íƒ€ê²Ÿ ë°ì´í„°ì˜ ì›-í•« ì¸ì½”ë”©\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_oh = to_categorical(y_train)\n",
        "y_val_oh = to_categorical(y_val)\n",
        "y_test_oh = to_categorical(y_test)\n",
        "\n",
        "print('y_train_oh : ', y_train_oh.shape)\n",
        "print('y_val_oh : ', y_val_oh.shape)\n",
        "print('y_test_oh : ', y_test_oh.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë“œë¡­ì•„ì›ƒ ë ˆë¦¬ì–´ë¥¼ ì¶”ê°€í•œ ëª¨ë¸ êµ¬ì„±\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def build_dropout_model():\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv2D(filters=16, kernel_size= 3,\n",
        "         strides=(1, 1), padding='same', activation='relu',\n",
        "         input_shape=(28, 28, 1)))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(filters=32, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv2D(filters=64, kernel_size= 3,\n",
        "        strides=(1, 1), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "  return model\n",
        "model = build_dropout_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IRrZ-TRe1oAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì»´íŒŒì¼ ë° í•™ìŠµ\n",
        "# Make sure to run cell KQ61MEfEkxPk to build the model before running this cell.\n",
        "model.compile(optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['acc'])\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "           epochs = EPOCHS,\n",
        "           batch_size = BATCH_SIZE,\n",
        "           validation_data = (X_val, y_val_oh),\n",
        "           verbose = 1)"
      ],
      "metadata": {
        "id": "RCCiWryZ1pF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "I6XYPuiS2Qhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de545b7f"
      },
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "def plot_history(history):\n",
        " hist = pd.DataFrame(history.history)\n",
        " hist['epoch'] = history.epoch\n",
        " plt.figure(figsize=(16,8))\n",
        " plt.subplot(1,2,1)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Loss')\n",
        " plt.plot(hist['epoch'], hist['loss'], label='Train Loss')\n",
        " plt.plot(hist['epoch'], hist['val_loss'],label = 'Val Loss')\n",
        " plt.legend()\n",
        " plt.subplot(1,2,2)\n",
        " plt.xlabel('Epoch')\n",
        " plt.ylabel('Accuracy')\n",
        " plt.plot(hist['epoch'], hist['acc'], label='Train Accuracy')\n",
        " plt.plot(hist['epoch'], hist['val_acc'], label = 'Val Accuracy')\n",
        " plt.legend()\n",
        " plt.show() # Add plt.show() here\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì¸¡\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_argmax = np.argmax(y_pred, axis=1)\n",
        "y_pred_argmax[:10]"
      ],
      "metadata": {
        "id": "JmwzGC-p2fn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ì§€í‘œê³„ì‚°\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "def print_score(y_test, y_pred):\n",
        " print('accuracy: %.3f' % (accuracy_score(y_test, y_pred)))\n",
        " print('precision: %.3f' % (precision_score(y_test, y_pred,\n",
        "average='macro')))\n",
        " print('recall_score: %.3f' % (recall_score(y_test, y_pred,\n",
        "average='macro')))\n",
        " print('f1_score: %.3f' % (f1_score(y_test, y_pred,\n",
        "average='macro')))\n",
        "\n",
        "print_score(y_test, y_pred_argmax)"
      ],
      "metadata": {
        "id": "FHweudMb2mYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì»¬ëŸ¬ ì´ë¯¸ì§€ì˜ ë¶„ë¥˜\n",
        "\n"
      ],
      "metadata": {
        "id": "BaWddLyU53fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "FSYYW3od58IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° í‘œì‹œ\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "samples = np.random.randint(len(X_train), size=10)\n",
        "plt.figure(figsize=(12, 5))\n",
        "for i, idx in enumerate(samples):\n",
        "  plt.subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "  plt.title((class_names[y_train[idx][0]]))\n",
        "  plt.imshow(X_train[idx])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vuiiw6LY5-Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì •ê·œí™” ì²˜ë¦¬ -    ìµœì†Œ-ìµœëŒ€\tì •ê·œí™”\tì²˜ë¦¬\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "fFWxtxJi6WYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²€ì¦ìš© ë°ì´í„° ë¶„ë¦¬\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "           X_train, y_train, test_size = 0.3,  random_state = 42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "nHcyFGT36dNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë ˆì´ë¸” ë°ì´í„° shape ë³€ê²½ - ë ˆì´ë¸” ë°ì´í„°ë¥¼ 2ì°¨ì›ì—ì„œ 1ì°¨ì›ìœ¼ë¡œ ë³€ê²½\n",
        "y_train = y_train.reshape(-1)\n",
        "y_val = y_val.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "tfAFwZMU6qhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì›-í•« ì¸ì½”ë”©\n",
        "import tensorflow as tf\n",
        "y_train_oh = tf.one_hot(y_train, depth=10)\n",
        "y_val_oh = tf.one_hot(y_val, depth=10)\n",
        "y_test_oh = tf.one_hot(y_test, depth=10)\n",
        "print(y_train_oh.shape)\n",
        "y_train_oh[:5]"
      ],
      "metadata": {
        "id": "OcZpBISz6z-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜\n",
        "y_train_oh = y_train_oh.numpy()\n",
        "y_val_oh = y_val_oh.numpy()\n",
        "y_test_oh = y_test_oh.numpy()\n",
        "print(y_train_oh.shape)\n",
        "print(y_val_oh.shape)\n",
        "print(y_test_oh.shape)"
      ],
      "metadata": {
        "id": "h2lEyxw2647G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Conv2D(32, 3, padding = 'same',\n",
        "        activation='relu', input_shape = (32, 32, 3)))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(64, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(256, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(256, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HIPOW-Bg69BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=adam,\n",
        "         loss = 'categorical_crossentropy',\n",
        "        metrics=['acc'])\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "          epochs = EPOCHS,\n",
        "          batch_size = BATCH_SIZE,\n",
        "          validation_data = (X_val, y_val_oh))"
      ],
      "metadata": {
        "id": "JayjkH1o7EpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "PUC2zh367JRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì˜ˆì¸¡\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_argmax = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "HI_VQ39p7cNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í˜¼ë™í–‰ë ¬ ì‹œê°í™”\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "def plot_matrix(y_test, y_pred):\n",
        "  plt.figure(figsize = (10, 8))\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(cm, annot = True, fmt = 'd',cmap = 'Blues')\n",
        "  plt.xlabel('predicted label', fontsize = 15)\n",
        "  plt.ylabel('true label', fontsize = 15)\n",
        "  plt.show()\n",
        "\n",
        "plot_matrix(y_test, y_pred_argmax)"
      ],
      "metadata": {
        "id": "abMi1sBr7fYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì½œë°±í•¨ìˆ˜ ì ìš©"
      ],
      "metadata": {
        "id": "VC02Ix0tnz7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # Import tensorflow as tf\n",
        "from tensorflow import keras # Import keras\n",
        "\n",
        "model = build_model()\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=adam,\n",
        "         loss = 'categorical_crossentropy',\n",
        "         metrics=['acc'])"
      ],
      "metadata": {
        "id": "KY1Kobaunyjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import callbacks\n",
        "checkpoint_path = 'temp/cifar_10.weights.h5'\n",
        "checkpoint = callbacks.ModelCheckpoint(checkpoint_path,\n",
        "               save_weights_only=True,\n",
        "               save_best_only=True,\n",
        "               monitor='val_loss')"
      ],
      "metadata": {
        "id": "CspdJ9FLn5WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "           epochs = EPOCHS,\n",
        "           batch_size = BATCH_SIZE,\n",
        "           validation_data = (X_val, y_val_oh),\n",
        "           callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "VU5e8ouLoxrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ìœ„ì˜ ì˜¤ë¥˜ í•´ê²°ë²•\n"
      ],
      "metadata": {
        "id": "WnxIoG_jpKue"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76adecb3"
      },
      "source": [
        "# Load CIFAR-10 dataset\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b90fb290"
      },
      "source": [
        "# Normalize image data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e822e874"
      },
      "source": [
        "# Split validation data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "           X_train, y_train, test_size = 0.3,  random_state = 42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5f4a14e"
      },
      "source": [
        "# Reshape labels\n",
        "y_train = y_train.reshape(-1)\n",
        "y_val = y_val.reshape(-1)\n",
        "y_test = y_test.reshape(-1)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5bc6887"
      },
      "source": [
        "# One-hot encode labels\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_oh = to_categorical(y_train)\n",
        "y_val_oh = to_categorical(y_val)\n",
        "y_test_oh = to_categorical(y_test)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "y_train_oh[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514bdd4c"
      },
      "source": [
        "# Build the model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Conv2D(32, 3, padding = 'same',\n",
        "        activation='relu', input_shape = (32, 32, 3)))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(64, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(256, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Conv2D(256, 3, padding = 'same',\n",
        "        activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215ad210"
      },
      "source": [
        "# Define ModelCheckpoint callback\n",
        "from tensorflow.keras import callbacks\n",
        "checkpoint_path = 'temp/cifar_10.weights.h5'\n",
        "checkpoint = callbacks.ModelCheckpoint(checkpoint_path,\n",
        "               save_weights_only=True,\n",
        "               save_best_only=True,\n",
        "               monitor='val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19a3c73"
      },
      "source": [
        "# Compile and train the model\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=adam,\n",
        "         loss = 'categorical_crossentropy',\n",
        "        metrics=['acc'])\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "          epochs = EPOCHS,\n",
        "          batch_size = BATCH_SIZE,\n",
        "          validation_data = (X_val, y_val_oh),\n",
        "          callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒíƒœ ë³µì› - íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ìˆëŠ” ìµœì‚¬ìœ¼ì´ ìƒíƒœ(ê°€ì¤‘ì¹˜)ë¥¼ ëª¨ë¸ì— ë³µì›í•œë‹¤.\n",
        "model.load_weights(checkpoint_path)\n"
      ],
      "metadata": {
        "id": "-IQ9QoaHqobo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°°ì¹˜ ì •ê·œí™”\n",
        "- ê³¼ëŒ€\tì í•©ì„\të°©ì§€í•˜ëŠ”\tíš¨ê³¼"
      ],
      "metadata": {
        "id": "KWFO5gnQroYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ êµ¬ì„±\n",
        "def build_model_bach_normalization():\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, 3, padding = 'same', input_shape =\n",
        "(32, 32, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "    model.add(layers.Conv2D(64, 3, padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "    model.add(layers.Conv2D(256, 3, padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "    model.add(layers.Conv2D(256, 3, padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.MaxPooling2D(2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "z01NyKeQrsbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model_bach_normalization()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "F-sEsOxNr56v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=adam,\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "1TZZFzJMr9-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "history = model.fit(X_train, y_train_oh,\n",
        "                    epochs = EPOCHS,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    validation_data = (X_val, y_val_oh))"
      ],
      "metadata": {
        "id": "YUFnaRTKsCIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ í‰ê°€\n",
        "loss, acc = model.evaluate(X_test, y_test_oh)\n",
        "print('loss : ', loss)\n",
        "print('acc : ', acc)"
      ],
      "metadata": {
        "id": "DT8XzHSosPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„° ì¦ê°•\n",
        "- ê¸°ì¡´ì˜ ë°ì´í„°ë¥¼ ë³€í˜•í•´ì„œ ë°ì´í„°ì˜ ì–‘ì„ ëŠ˜ë¦¬ëŠ” ê²ƒ\n"
      ],
      "metadata": {
        "id": "gdsS6jSqstPO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROplzPRMsxYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì „ì´í•™ìŠµ (Transfer Learning)\n",
        "- ëŒ€ìš©ëŸ‰ì˜\të°ì´í„°\tì„¸íŠ¸ì—ì„œ\tí•™ìŠµ\tì™„ë£Œëœ\tëª¨ë¸(VGG16,\tResnet50\të“±)ì„\tì´ìš©í•´ì„œ\tìš°ë¦¬ê°€\tí•´ê²°í•˜ë ¤\n",
        "ëŠ”\të¬¸ì œì—\tì ìš©í•˜ê¸°\tìœ„í•´\të¯¸ì„¸\tì¡°ì •(fine\ttuning)í•˜ëŠ”\tê²ƒ"
      ],
      "metadata": {
        "id": "gBNpBeV1uiQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "CGpNuZSlurGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ì¤€ë¹„\n",
        "from tensorflow.keras.applications import VGG16\n",
        "vgg16 = VGG16(weights = 'imagenet',\n",
        "             input_shape = (32, 32, 3),\n",
        "             include_top = False)\n",
        "vgg16.trainable=False"
      ],
      "metadata": {
        "id": "JXF4vTb7uuQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶„ë¥˜ ë¶€ë¶„ì˜ ëª¨ë¸ êµ¬ì„±\n",
        "model = keras.Sequential()\n",
        "model.add(vgg16)\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "8TFrb8mxu04s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶„ë¥˜ ë¶€ë¶„ì˜ ëª¨ë¸ êµ¬ì„±\n",
        "model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
        "       loss = 'categorical_crossentropy',\n",
        "       metrics = ['acc'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vJ3HMgOWu6St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½œë°± ìƒì„±\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "lrr = ReduceLROnPlateau(monitor='val_acc',\n",
        "             patience=3,\n",
        "             verbose=1,\n",
        "             factor=0.8,\n",
        "             min_lr=0.0001)"
      ],
      "metadata": {
        "id": "ft5IkBqRu_ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°”ìš´ë”© ë°•ìŠ¤ ê·¼ì‚¬\n"
      ],
      "metadata": {
        "id": "bco4eOWpwKoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "def xml_to_csv(path):\n",
        "  xml_list = []\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      bbx = member.find('bndbox')\n",
        "      xmin = int(bbx.find('xmin').text)\n",
        "      ymin = int(bbx.find('ymin').text)\n",
        "      xmax = int(bbx.find('xmax').text)\n",
        "      ymax = int(bbx.find('ymax').text)\n",
        "      label = member.find('name').text\n",
        "      value = (root.find('filename').text,\n",
        "            int(root.find('size')[0].text),\n",
        "            int(root.find('size')[1].text),\n",
        "            label, xmin, ymin, xmax, ymax)\n",
        "      xml_list.append(value)\n",
        "\n",
        "  column_name = ['filename', 'width', 'height',\n",
        "           'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "  return xml_df"
      ],
      "metadata": {
        "id": "FJwAN446vIGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define IMAGE_PATH - **Replace '/content/your_image_directory' with the actual path to your XML files**\n",
        "IMAGE_PATH = '/content/your_image_directory'\n",
        "\n",
        "xml_df = xml_to_csv(IMAGE_PATH)\n",
        "csv_path = os.path.join(IMAGE_PATH, 'labels_cats.csv')\n",
        "xml_df.to_csv(csv_path, index=None)\n",
        "print('csv path:', csv_path)"
      ],
      "metadata": {
        "id": "5gw_Uk6kwTmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ì›ë˜ íŒŒì¼ëª…\n",
        "original_path = \"/content/(20')CCTVí•™ìŠµë°ì´í„°ì…‹(35,200ì¥).egg\"\n",
        "\n",
        "# ìƒˆ ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
        "new_path = \"/content/cctv_dataset.egg\"\n",
        "os.rename(original_path, new_path)\n"
      ],
      "metadata": {
        "id": "CXsnt3QqweDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ìš°ì„  unzip ì‹œë„\n",
        "!unzip -q /content/cctv_dataset.egg -d /content/cctv_data\n"
      ],
      "metadata": {
        "id": "mDI4q8dGyYDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ì••ì¶• í•´ì œëœ í´ë” êµ¬ì¡° í™•ì¸\n",
        "for root, dirs, files in os.walk(\"/content/cctv_data\"):\n",
        "    print(f\"ğŸ“ {root}\")\n",
        "    for f in files[:10]:  # ì²˜ìŒ 10ê°œë§Œ ë³´ê¸°\n",
        "        print(\"  â””â”€\", f)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "ZtwpiDPIyZHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}