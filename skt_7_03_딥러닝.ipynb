{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5uQz4diui5b76kw8gQbrC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park-hoyeon/park-hoyeon.github.io/blob/master/skt_7_03_%EB%94%A5%EB%9F%AC%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://localhost:8888/notebooks/Downloads/python_1.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "CylqI_ic5k1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 신경망 맛보기"
      ],
      "metadata": {
        "id": "7pFNhatF550f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "pTxPi3gY5tSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nJzZrtM974jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset  = torchvision.datasets.FashionMNIST(\"data/minst\", download=True, transform=\n",
        "                                                transforms.Compose([transforms.ToTensor()]))\n",
        "test_dataset  = torchvision.datasets.FashionMNIST(\"data/minst\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()]))"
      ],
      "metadata": {
        "id": "9lS184RSmaet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=100)"
      ],
      "metadata": {
        "id": "dUeKhB0SmcJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
        "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'}\n",
        "\n",
        "fig = plt.figure(figsize=(8,8));\n",
        "columns = 4;\n",
        "rows = 5;\n",
        "for i in range(1, columns*rows +1):\n",
        "    img_xy = np.random.randint(len(train_dataset));\n",
        "    img = train_dataset[img_xy][0][0,:,:]\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.title(labels_map[train_dataset[img_xy][1]])\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2TzzfBOmet4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionDNN,self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=784,out_features=256) # 28*28 의 이미지 특징 추출\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=256,out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128,out_features=10)\n",
        "\n",
        "    def forward(self,input_data):\n",
        "        out = input_data.view(-1, 784)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.drop(out)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_bh2e6uPmgqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001;\n",
        "model = FashionDNN();\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
        "print(model)"
      ],
      "metadata": {
        "id": "yIVwedbSmln2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        train = images.view(100, 1, 28, 28)\n",
        "        labels = labels\n",
        "\n",
        "        outputs = model(train)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "\n",
        "        if not (count % 50):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels_list.append(labels)\n",
        "                test = images.view(100, 1, 28, 28)\n",
        "                outputs = model(test)\n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                predictions_list.append(predictions)\n",
        "                correct += (predictions == labels).sum()\n",
        "                total += len(labels)\n",
        "\n",
        "            accuracy = correct * 100 / total\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        if not (count % 500):\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "id": "dS2twvwymm7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "M7XY5tm8mpWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001;\n",
        "model = FashionCNN();\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss();\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);\n",
        "print(model)"
      ],
      "metadata": {
        "id": "hhfAZl6znXtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        train = Variable(images.view(100, 1, 28, 28))\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        outputs = model(train)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "\n",
        "        if not (count % 50):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels_list.append(labels)\n",
        "                test = Variable(images.view(100, 1, 28, 28))\n",
        "                outputs = model(test)\n",
        "                predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                predictions_list.append(predictions)\n",
        "                correct += (predictions == labels).sum()\n",
        "                total += len(labels)\n",
        "\n",
        "            accuracy = correct * 100 / total\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        if not (count % 500):\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))"
      ],
      "metadata": {
        "id": "z-6PeDimnZfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 특성 추출 기법\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1_nho4O6ncmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QcDJ52HtnbCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip catanddog.zip -d data/"
      ],
      "metadata": {
        "id": "bdFg3d0Ttdmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir())  # 현재 디렉토리\n",
        "print(os.listdir('data'))  # data 디렉토리 내부\n"
      ],
      "metadata": {
        "id": "pQ7L8t0POzPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 압축 해제\n",
        "zip_path = \"/content/catanddog.zip\"\n",
        "extract_path = \"/content/catanddog\"  # 이 위치에 train/과 test/ 폴더가 생기게 할 것\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# 결과 확인\n",
        "print(\"압축 해제 완료. 존재하는 폴더:\", os.listdir(extract_path))\n"
      ],
      "metadata": {
        "id": "858F0kAZQqnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 실제 경로 설정\n",
        "data_path = '/content/catanddog/catanddog/train'\n",
        "\n",
        "# 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize([256, 256]),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 데이터셋\n",
        "train_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"학습 이미지 수:\", len(train_dataset))\n"
      ],
      "metadata": {
        "id": "Q2sTMOB0t1fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples, labels = next(iter(train_loader))\n",
        "classes = {0:'cat', 1:'dog'}\n",
        "fig = plt.figure(figsize=(16,24))\n",
        "for i in range(24):\n",
        "    a = fig.add_subplot(4,6,i+1)\n",
        "    a.set_title(classes[labels[i].item()])\n",
        "    a.axis('off')\n",
        "    a.imshow(np.transpose(samples[i].numpy(), (1,2,0)))\n",
        "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
      ],
      "metadata": {
        "id": "1w5VWu_tOYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "id": "ug_06XFZRHRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting=True):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "set_parameter_requires_grad(resnet18)"
      ],
      "metadata": {
        "id": "dXAx2ZP3RI_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.fc = nn.Linear(512, 2)"
      ],
      "metadata": {
        "id": "zaHloQtzRKe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in resnet18.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "id": "GRTPh7uURRdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = torch.nn.Linear(512, 2)\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(model.fc.parameters())\n",
        "cost = torch.nn.CrossEntropyLoss()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "5qEMEUSpRRrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=13, is_train=True):\n",
        "    since = time.time()\n",
        "    acc_history = []\n",
        "    loss_history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            model.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
        "\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "\n",
        "        acc_history.append(epoch_acc.item())\n",
        "        loss_history.append(epoch_loss)\n",
        "        torch.save(model.state_dict(), os.path.join('catanddog/', '{0:0=2d}.pth'.format(epoch)))\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Acc: {:4f}'.format(best_acc))\n",
        "    return acc_history, loss_history"
      ],
      "metadata": {
        "id": "Xqq4ZSMgRTC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name,param in resnet18.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)\n",
        "\n",
        "optimizer = optim.Adam(params_to_update)"
      ],
      "metadata": {
        "id": "ZhM0Ur2_RWJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_acc_hist, train_loss_hist = train_model(resnet18, train_loader, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "fQCV4DeGRXxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = 'catanddog/catanddog/test'\n",
        "\n",
        "transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize(224),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                ])\n",
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=test_path,\n",
        "    transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "YvG3LlrBRY_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, dataloaders, device):\n",
        "    since = time.time()\n",
        "    acc_history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    saved_models = glob.glob('catanddog/' + '*.pth')\n",
        "    saved_models.sort()\n",
        "    print('saved_model', saved_models)\n",
        "\n",
        "    for model_path in saved_models:\n",
        "        print('Loading model', model_path)\n",
        "\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in dataloaders:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            preds[preds >= 0.5] = 1\n",
        "            preds[preds < 0.5] = 0\n",
        "            running_corrects += preds.eq(labels).int().sum()\n",
        "\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
        "        print('Acc: {:.4f}'.format(epoch_acc))\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "\n",
        "        acc_history.append(epoch_acc.item())\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    return acc_history"
      ],
      "metadata": {
        "id": "mZIDRZErRtpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc_hist = eval_model(resnet18, test_loader, device)"
      ],
      "metadata": {
        "id": "MKKg6Mt2SBJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_acc_hist, label='Train Accuracy')\n",
        "plt.plot(val_acc_hist, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vJZt6_bYSCtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_hist, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYjrNUq8SXWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def im_convert(tensor):\n",
        "    image=tensor.clone().detach().numpy()\n",
        "    image=image.transpose(1,2,0)\n",
        "    image=image*(np.array((0.5,0.5,0.5))+np.array((0.5,0.5,0.5)))\n",
        "    image=image.clip(0,1)\n",
        "    return image"
      ],
      "metadata": {
        "id": "KOQDshA0SZHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {0:'cat', 1:'dog'}\n",
        "\n",
        "dataiter=iter(test_loader)\n",
        "images,labels=next(dataiter)\n",
        "output=model(images)\n",
        "_,preds=torch.max(output,1)\n",
        "\n",
        "fig=plt.figure(figsize=(25,4))\n",
        "for idx in np.arange(20):\n",
        "    ax=fig.add_subplot(2,10,idx+1,xticks=[],yticks=[])\n",
        "    plt.imshow(im_convert(images[idx]))\n",
        "    a.set_title(classes[labels[i].item()])\n",
        "    ax.set_title(\"{}({})\".format(str(classes[preds[idx].item()]),str(classes[labels[idx].item()])),color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "plt.show()\n",
        "plt.subplots_adjust(bottom=0.2, top=0.6, hspace=0)"
      ],
      "metadata": {
        "id": "B8z7fQHXSb0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp1LsB1NSc5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}