{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMSQLKXQo9b2rYgOEHDXsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/park-hoyeon/park-hoyeon.github.io/blob/master/skt_7_10_%EB%94%A5%EB%9F%AC%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WdEsEqdu2uy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models # 파이토치에서 제공하는 이미지 분야에 특화된 모델들을 불러옴\n",
        "# 미리 학습되지 않은(랜덤 초기화) ResNet-50\n",
        "resnet50 = models.resnet50(pretrained=False)\n",
        "# pretrained=False로 설정하면, 모델의 가중치(파라미터)가 무작위로 초기화되어 있어,\n",
        "# 즉, 아직 학습이 전혀 되지 않은 상태이므로, 처음부터 다시 학습(train)해야 함\n",
        "# 미리 학습된(pretrained) ResNet-50\n",
        "resnet50_pretrained = models.resnet50(pretrained=True)\n",
        "# pretrained=True로 설정하면, ImageNet 데이터셋(대략 100만 장 이상의 이미지, 1000개 클래스)으로 미리 학습된 가중치를 그대로 불러옴\n",
        "# 미리 학습된 모델을 사용하면, 적은 데이터로도 좋은 성능을 얻을 가능성이 높아짐. 이미 ‘일반적인 이미지 특징’을 잘 학습하고 있기 때문\n",
        "# 마지막 FC 레이어 수정 (예: 클래스 수 10개로 변경)\n",
        "num_features = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Linear(num_features, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bottleneck 블록 직접 구현하기\n"
      ],
      "metadata": {
        "id": "zQyXb9IxvECx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn # 신경망 네트워크(레이어, 손실 함수 등)를 구성하는 기본 모듈\n",
        "import torch.nn.functional as F # PyTorch에서 자주 쓰는 함수들(functional API). 예: F.relu, F.conv2d 등.\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4  # Bottleneck 확장 배수 (마지막 1x1 Conv를 통해 채널 수를 4배로 확장한다는 설정값)\n",
        "    # in_channels: 블록의 입력 특징 맵(feature map)의 채널 수. (이전 블록의 출력 채널 수)\n",
        "    # out_channels: 1x1과 3x3를 처리한 후, 마지막 단계에서 확장되기 전 기본 채널 수.\n",
        "    # stride: 합성곱의 보폭. 보통은 1이지만, 블록에 따라 2로 설정되어 크기를 절반으로 줄이는 역할을 할 수도 있음.\n",
        "    # downsample: skip connection에서 입력(identity)의 차원을 맞춰주기 위한 추가 모듈(합성곱+BN 등)이 들어갈 수 있음.\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        # 부모 클래스(nn.Module)의 초기화 메서드를 호출해 필요한 내부 구조를 설정\n",
        "        # 1x1 Conv\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        # bias=False: 합성곱 후에 BatchNorm을 바로 쓰면, 합성곱의 bias가 불필요하다고 판단하여 생략하는 경우가 많음\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) # Batch Normalization으로 학습을 안정화하고, 학습 속도를 높여준다. 왜?\n",
        "        # 3x3 Conv\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        # 1x1 Conv (채널 확장, 출력 채널을 out_channels * expansion(기본 4배)로 확장)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        # ResNet의 Bottleneck 구조는 이렇게 첫 번째 1x1 Conv에서 채널을 줄였다가(연산량 감소 효과),\n",
        "        # 3x3에서 특징 추출 후, 마지막에 다시 1x1 Conv로 채널을 확장\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        # skip connection에서 입력 x와 출력 out의 차원(채널 수 및 가로/세로 크기)이 다를 경우,\n",
        "        # 이 값이 합성곱+BN 모듈 등이 되어 identity를 변환해 줌.\n",
        "        # (예를 들어 블록의 stride=2로 크기가 절반이 되면, identity도 크기를 맞춰야 하기 때문)\n",
        "    def forward(self, x): # 블록에서 입력 x가 들어왔을 때, 어떤 계산을 할지 정의\n",
        "        identity = x # skip connection에 사용할 원본 입력을 identity라는 변수에 잠시 저장\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        # downsample이 있으면 skip connection의 차원을 맞춤\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity # skip connection(잔차 연결)\n",
        "        # ResNet의 핵심 아이디어는 입력(x)을 출력에 더해 모델이 잔차(residual) 만을 학습하도록 한 것\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "DCOXOae3vKDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 Bottleneck 코드는 이미지가 들어오면 채널을 줄였다가(conv1), 특징을 뽑고(conv2), 다시 채널을 늘린 다음(conv3), 마지막에 원래 입력 이미지를 더해주는(residual connection) 과정을 거치는 작은 신경망 블록을 만든 *것*"
      ],
      "metadata": {
        "id": "9EW0NqTu-JUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  컨볼루션 레이어 다음에 Batch Normalization(배치 정규화)을 사용하는 것\n",
        ": 신경망은 여러 층(레이어)을 쌓아서 만듭니다. 각 층은 이전 층의 출력을 입력으로 받아서 계산을 합니다.\n",
        "학습 과정에서 각 층의 파라미터(가중치, 편향)는 계속 변합니다.\n",
        "문제는 이전 층의 파라미터가 변하면, 현재 층으로 들어오는 입력 데이터의 분포(평균, 분산 등)가 계속 바뀌게 됩니다. 이게 마치 데이터의 '공변량'이 변하는 것 같다고 해서 '내부 공변량 변화'라고 부릅니다.\n",
        "이렇게 층마다 입력 데이터의 분포가 계속 바뀌면, 현재 층은 매번 새로운 분포에 맞춰서 학습을 해야 하므로 학습 속도가 느려지고, 학습이 불안정해지거나 심지어 수렴하지 못하는 경우도 생깁니다.\n",
        "\n",
        "--> 결론적으로, 컨볼루션 레이어 다음에 Batch Normalization을 사용하는 것은 컨볼루션 연산의 결과를 안정화시켜서, 다음 레이어가 더 일관된 데이터를 받아 효율적이고 안정적으로 학습할 수 있도록 돕기 위함"
      ],
      "metadata": {
        "id": "t5ugic55-sq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50 전체 구조 구현"
      ],
      "metadata": {
        "id": "k-P5YTCh8HFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        # 초기 Stem (보통 ResNet에서는 첫 번째 7x7 Conv + MaxPool을 거쳐 이미지 크기를 빠르게 줄이는 과정을 Stem이라고 부름)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # inplanes(현재 블록의 입력 채널 수를 추적하는 변수) 설정\n",
        "        # 처음에는 64(Stem의 출력 채널)로 설정.\n",
        "        self.inplanes = 64\n",
        "        # 레이어 구성\n",
        "        self.layer1 = self._make_layer(Bottleneck, 64,  3, stride=1)\n",
        "        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(Bottleneck, 256, 6, stride=2)\n",
        "        self.layer4 = self._make_layer(Bottleneck, 512, 3, stride=2)\n",
        "        # 분류기(Head) 부분\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # 입력 Feature Map의 크기와 상관없이 (1, 1) 크기로 만듦\n",
        "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        # stride!=1이거나, 채널 수가 맞지 않으면 다운샘플 구성\n",
        "        # 다운샘플(downsample): stride가 2가 되면(또는 채널 수가 달라지면),\n",
        "        # skip connection에서 입력 x와 출력의 크기를 맞춰줘야 함\n",
        "        # downsample은 1x1 Conv와 BN으로 구성되며, stride를 조절해 공간 크기를 줄이거나, 채널 수를 맞춰줌\n",
        "        if stride != 1 or self.inplanes != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, out_channels * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion)\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, out_channels, stride, downsample)) # 입력이 줄어들거나 채널이 변경되는 지점\n",
        "        self.inplanes = out_channels * block.expansion # self.inplanes를 업데이트 (새로운 블록의 출력 채널)\n",
        "        # 나머지 블록은 stride=1 이므로 downsample 없이 이어짐\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, out_channels))\n",
        "        return nn.Sequential(*layers) # 마지막에 nn.Sequential(*layers)로 묶어 하나의 큰 레이어로 반환\n",
        "    def forward(self, x):\n",
        "        # Stem\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "          # 4개의 레이어\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # 분류\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "# 모델 예시 생성 (최종 출력)\n",
        "model = MyResNet50(num_classes=1000)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "tuh704jbvQmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Model\n"
      ],
      "metadata": {
        "id": "LX8lCNJORcdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.draw import disk, rectangle"
      ],
      "metadata": {
        "id": "IfmN9Rkn8Kva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate an image with shapes and labels\n",
        "def create_image_with_shapes_and_labels(image_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Creates a dummy RGB image with shapes (circle, rectangle) and corresponding labels.\n",
        "    Args:\n",
        "        image_size (tuple): Size of the image (H, W).\n",
        "    Returns:\n",
        "        torch.Tensor: RGB image tensor (3, H, W).\n",
        "        torch.Tensor: Label tensor (H, W) with classes 0 (background), 1 (circle), 2 (rectangle).\n",
        "    \"\"\"\n",
        "    image = np.zeros((*image_size, 3), dtype=np.float32)  # 높이 × 너비 × RGB(3채널)로 된 검정색 빈 이미지 생성\n",
        "    label = np.zeros(image_size, dtype=np.int64)  # 같은 크기의 라벨(정답) 배열도 만듦. 초기값은 전부 배경 (0)\n",
        "    # Draw a circle\n",
        "    rr, cc = disk((64, 64), 40)\n",
        "    image[rr, cc, 0] = 1.0  # Red circle\n",
        "    label[rr, cc] = 1  # Class 1: Circle\n",
        "    # Draw a rectangle\n",
        "    start = (120, 120)\n",
        "    extent = (50, 80)\n",
        "    rr, cc = rectangle(start=start, extent=extent)\n",
        "    image[rr, cc, 1] = 1.0  # Green rectangle\n",
        "    label[rr, cc] = 2  # Class 2: Rectangle\n",
        "    # Normalize to range [0, 1] - 이미지 정규화\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    return torch.tensor(image).permute(2, 0, 1), torch.tensor(label)  # (C, H, W), (H, W)"
      ],
      "metadata": {
        "id": "fI9XV-_LRg2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a basic double convolution block\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False), # 패딩이 1이라서 이미지 크기는 유지됨\n",
        "        nn.BatchNorm2d(out_channels),# 출력 채널별로 정규화해줌 → 학습 안정화, 속도 향상\n",
        "\n",
        "\n",
        "        nn.ReLU(inplace=True),# 비선형성 부여\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True), # 출력 채널 수를 유지하면서 한 번 더 특징을 뽑아냄 (이중 컨볼루션으로 더 깊은 특성 추출 가능)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "dmOlQarPRiOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과적으로 이 블록은?<br>\n",
        "3×3 conv 2번 → receptive field가 커짐 (간접적으로 5×5 효과)<br>\n",
        "\n",
        "BatchNorm + ReLU 2번 → 안정적이고 표현력 있는 특징 생성<br>\n",
        "\n",
        "U-Net에서 반복 사용되며, Encoder와 Decoder 모두에 핵심 구성 요소<br>"
      ],
      "metadata": {
        "id": "X7Qc1PnooP1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the U-Net model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        # Encoder - 채널 수를 점점 늘려가며 깊은 특징 추출\n",
        "        self.enc1 = double_conv(in_channels, 64)\n",
        "        self.enc2 = double_conv(64, 128)\n",
        "        self.enc3 = double_conv(128, 256)\n",
        "        self.enc4 = double_conv(256, 512)\n",
        "\n",
        "        # Bottleneck - 가장 깊은 특징 추출 구간 (이미지 크기 가장 작음)\n",
        "        # U-Net 중앙 - 채널 수가 가장 큼\n",
        "        self.bottleneck = double_conv(512, 1024)\n",
        "\n",
        "        # Decoder - Encoder의 출력을 cat으로 이어붙여 경계 보존 + 이후 다시 double_conv로 정제\n",
        "        # ConvTranspose2d: 업샘플링 (해상도 2배 증가)\n",
        "        # double_conv: 합친 결과를 다시 정제 (잡음 제거, 경계 보존)\n",
        "        # torch.cat(enc, dec): 같은 레벨의 인코더 출력과 합침\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = double_conv(1024, 512)\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = double_conv(512, 256)\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = double_conv(256, 128)\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = double_conv(128, 64)\n",
        "\n",
        "        # Output layer - 마지막 채널 수를 클래스 수로 줄임 (예: 3개의 클래스라면 3 채널)\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Encoder - Pooling으로 해상도는 줄이고, 채널은 증가\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(nn.MaxPool2d(kernel_size=2)(enc1))\n",
        "        enc3 = self.enc3(nn.MaxPool2d(kernel_size=2)(enc2))\n",
        "        enc4 = self.enc4(nn.MaxPool2d(kernel_size=2)(enc3))\n",
        "           # Bottleneck\n",
        "        bottleneck = self.bottleneck(nn.MaxPool2d(kernel_size=2)(enc4))\n",
        "\n",
        "        # Decoder - 업샘플링 (ConvTranspose2d) → 인코더의 같은 단계와 concat -> 경계 정보 복원, 더 세밀한 결과 생성\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((enc4, dec4), dim=1) # upconv4: Transposed Convolution을 사용해 업샘플링 (크기 2배로 키움)\n",
        "        dec4 = self.dec4(dec4) # torch.cat: 인코더(enc4)의 출력과 디코더(dec4)를 채널 방향으로 이어 붙임\n",
        "        dec3 = self.upconv3(dec4) # dec4: double_conv 블록으로 특징 다시 정리 (경계 보존 + 채널 축소)\n",
        "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
        "        dec3 = self.dec3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
        "        dec2 = self.dec2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
        "        dec1 = self.dec1(dec1)\n",
        "\n",
        "        # Output - 픽셀마다 클래스 score 예측 → 최종 마스크 결과\n",
        "        out = self.out_conv(dec1)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "XPI3QQ4qRjqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "① Encoder (인코더)\t이미지를 점점 작게 만들면서 특징 추출 <br>\n",
        "② Bottleneck (중앙 통로)\t정보 압축 구간 (특징 요약) <br>\n",
        "③ Decoder (디코더)\t이미지를 다시 원래 크기로 복원 (픽셀별 예측) <br>\n",
        "\n",
        "### 🔽 디코더 레이어 4 (가장 깊은 층부터 복원 시작)\n",
        "dec4 = self.upconv4(bottleneck)  # 1024 → 512 채널로 업샘플링 (공간 크기 2배) <br>\n",
        "dec4 = torch.cat((enc4, dec4), dim=1)  # enc4와 연결: 채널 512 + 512 → 1024 <br>\n",
        "dec4 = self.dec4(dec4)  # 1024 → 512 채널로 다시 정리<br>\n"
      ],
      "metadata": {
        "id": "yGoom0e1T4fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, optimizer, criterion, num_epochs, input_image, ground_truth):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad() # 이전 step에서 계산된 gradient 초기화\n",
        "        # Forward pass\n",
        "        outputs = model(input_image) #이미지 입력 → 모델이 예측한 마스크 outputs 생성\n",
        "        loss = criterion(outputs, ground_truth.unsqueeze(0))  # ground_truth에 .unsqueeze(0)을 적용해 배치 차원 (1, H, W)로 만듦\n",
        "        # Backward pass - 손실을 모델 파라미터로 미분\n",
        "        loss.backward()\n",
        "        optimizer.step() # 계산된 gradient를 바탕으로 모델 가중치를 한 번 업데이트\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "    print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "GLp3GgPERlUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of results\n",
        "def visualize_results(input_image, output_prediction, ground_truth=None):\n",
        "    # 배치 차원 제거 (squeeze()) → (3, H, W)\n",
        "    # 채널 순서 변경 (permute) → (H, W, 3) → 이미지로 보이게 함\n",
        "    # cpu().numpy()로 넘파이 배열로 변환\n",
        "    input_image = input_image.squeeze().permute(1, 2, 0).cpu().numpy()  # Convert to HWC\n",
        "\n",
        "    # argmax로 가장 높은 확률의 클래스 선택 -> 각 픽셀에 대해 가장 가능성 높은 클래스 번호만 추출\n",
        "    output_prediction = torch.argmax(output_prediction, dim=1).squeeze().cpu().numpy()  # Convert to label map\n",
        "    if ground_truth is not None:\n",
        "        ground_truth = ground_truth.cpu().numpy()\n",
        "\n",
        "    # Plot the images - 1행 3열의 subplot 만들기 (이미지, 예측, 정답)\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    # 첫 번째: 원본 이미지 보여주기\n",
        "    ax[0].imshow(input_image)\n",
        "    ax[0].set_title(\"Input Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "    # 두 번째: 모델이 예측한 분할 결과 (클래스별 색) 보여주기\n",
        "    ax[1].imshow(output_prediction, cmap=\"jet\")\n",
        "    ax[1].set_title(\"Model Prediction\")\n",
        "    ax[1].axis(\"off\")\n",
        "    if ground_truth is not None: # 세 번째: 정답 레이블이 있는 경우 같이 비교해보기\n",
        "        ax[2].imshow(ground_truth, cmap=\"jet\")\n",
        "        ax[2].set_title(\"Ground Truth\")\n",
        "        ax[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LUQfiFNIRm79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create synthetic data\n",
        "    input_image, ground_truth = create_image_with_shapes_and_labels()\n",
        "    input_image = input_image.unsqueeze(0)  # Add batch dimension - PyTorch는 (Batch, Channel, Height, Width) 순서를 기대하므로 unsqueeze(0)으로 배치 차원 추가 → (1, 3, 256, 256)\n",
        "    ground_truth = ground_truth  # (H, W)\n",
        "    # Instantiate U-Net model\n",
        "    num_classes = 3  # Background, Circle, Rectangle\n",
        "    #model = UNet(in_channels=3, out_channels=num_classes)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 입력 채널은 RGB(3), 출력 채널은 클래스 수(배경, 원, 사각형 = 3)\n",
        "    model = UNet(in_channels=3, out_channels=num_classes).to(device)\n",
        "    input_image = input_image.to(device)\n",
        "    ground_truth = ground_truth.to(device)\n",
        "\n",
        "    # Define loss and opt\n",
        "    criterion = nn.CrossEntropyLoss() # CrossEntropyLoss: 다중 클래스 분류 문제에서 사용\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, optimizer, criterion, num_epochs=10, input_image=input_image, ground_truth=ground_truth)\n",
        "\n",
        "    # Evaluate the model - .eval()은 드롭아웃/배치정규화 등을 평가 모드로 전환\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_prediction = model(input_image)\n",
        "    # Visualize results\n",
        "    visualize_results(input_image.squeeze(), output_prediction, ground_truth) #squeeze()는 (1, 3, H, W) → (3, H, W)로 배치 차원 제거"
      ],
      "metadata": {
        "id": "Qru-hNOMRxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eWnLLTdknHDk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERYM778FRzTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
